{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2019.11.02. 6.3 simple RNN Training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AABw7-g7Am44","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNpNRNVGA_r4","colab_type":"code","colab":{}},"source":["n_hidden = 35\n","lr = 0.01\n","epochs = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"57FhPw-3BFhW","colab_type":"code","colab":{}},"source":["string = 'hello pytorch. how long can a rnn cell remember? show me your limit!'\n","chars = 'abcdefghijklmnopqrstuvwxyz ?!.,:;01'\n","\n","# 문자를 리스트로 바꾸고 이의 길이를 저장.\n","char_list = [i for i in chars]\n","n_letters = len(char_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kL7erChBa-2","colab_type":"code","colab":{}},"source":["# string on-hot\n","def string_to_onehot(strong):\n","  start = np.zeros(shape = n_letters, dtype = int)\n","  end = np.zeros(shape = n_letters, dtype = int)\n","  start[-2] = 1\n","  end[-1] = 1\n","\n","  # 문자열을 차례대로 받아서 진행\n","  for i in string:\n","    # 들어온 문자가 몇 번째 문자인지 확인\n","    idx = char_list.index(i)\n","    # 0으로 배열 생성\n","    zero = np.zeros(shape = n_letters, dtype = int)\n","\n","    # 해당 문자만 1로\n","    zero[idx] = 1\n","    # start와 새로 생긴 문자를 붙이고 start에 할당\n","    # 반복하면 h의 one hot, e의 one hot.. 점점 쌓이게 됨\n","    start = np.vstack([start, zero])\n","  output = np.vstack([start, end])\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KPS4kZXOCiHR","colab_type":"code","colab":{}},"source":["def onehot_to_word(onehot_1):\n","  # 텐서를 입력받고 넘파이로 바꿔줌\n","  onehot = torch.Tensor.numpy(onehot_1)\n","  # onehot 벡터 최대값 위치로 인덱스를 찾음\n","  return char_list[onehot.argmax()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6PQu3FzDW8Q","colab_type":"code","colab":{}},"source":["# RNN with 1 hidden layer\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(RNN, self).__init__()\n","        \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        \n","        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n","        self.act_fn = nn.Tanh()\n","    \n","    def forward(self, input, hidden):\n","        # 입력과 hidden state를 cat함수로 붙여줍니다.\n","        combined = torch.cat((input, hidden), 1)\n","        # 붙인 값을 i2h 및 i2o에 통과시켜 hidden state는 업데이트, 결과값은 계산해줍니다.\n","        hidden = self.act_fn(self.i2h(combined))\n","        output = self.i2o(combined)\n","        return output, hidden\n","    \n","    # 아직 입력이 없을때(t=0)의 hidden state를 초기화해줍니다. \n","    def init_hidden(self):\n","        return torch.zeros(1, self.hidden_size)\n","    \n","rnn = RNN(n_letters, n_hidden, n_letters)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAmE5IMCEtIY","colab_type":"code","colab":{}},"source":["# 손실함수와 최적화함수를 설정\n","\n","loss_func = nn.MSELoss()\n","optimizer = torch.optim.Adam(rnn.parameters(), lr = lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yESK2EV6FKXf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a3565ca3-f730-4280-9dc2-2969f9c518cf","executionInfo":{"status":"ok","timestamp":1572681356535,"user_tz":-540,"elapsed":21082,"user":{"displayName":"김태웅","photoUrl":"","userId":"06196795162879697734"}}},"source":["one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n","\n","for i in range(epochs):\n","  optimizer.zero_grad()\n","  # 학습에 앞서 hidden state 초기화\n","  hidden = rnn.init_hidden()\n","\n","  # 문자열 전체에 대한 손실을 구하기 위해 total_loss 만들어줌\n","  total_loss = 0\n","  for j in range(one_hot.size()[0] - 1):\n","    # 입력은 앞의 글자\n","    # p y t o r c\n","    input_ = one_hot[j : j + 1, :]\n","    # 목표값은 뒤의 글자\n","    # y t o r c h\n","    target = one_hot[j + 1]\n","    output, hidden = rnn.forward(input_, hidden)\n","\n","    # 다 1차원으로 만듦\n","    loss = loss_func(output.view(-1), target.view(-1))\n","    total_loss += loss\n","\n","  total_loss.backward()\n","  optimizer.step()\n","\n","  if i % 10 == 0:\n","    print(total_loss)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor(2.8486, grad_fn=<AddBackward0>)\n","tensor(1.2048, grad_fn=<AddBackward0>)\n","tensor(0.6914, grad_fn=<AddBackward0>)\n","tensor(0.4150, grad_fn=<AddBackward0>)\n","tensor(0.2740, grad_fn=<AddBackward0>)\n","tensor(0.2012, grad_fn=<AddBackward0>)\n","tensor(0.1492, grad_fn=<AddBackward0>)\n","tensor(0.1236, grad_fn=<AddBackward0>)\n","tensor(0.0960, grad_fn=<AddBackward0>)\n","tensor(0.0857, grad_fn=<AddBackward0>)\n","tensor(0.0675, grad_fn=<AddBackward0>)\n","tensor(0.0621, grad_fn=<AddBackward0>)\n","tensor(0.0506, grad_fn=<AddBackward0>)\n","tensor(0.0452, grad_fn=<AddBackward0>)\n","tensor(0.0401, grad_fn=<AddBackward0>)\n","tensor(0.0425, grad_fn=<AddBackward0>)\n","tensor(0.0342, grad_fn=<AddBackward0>)\n","tensor(0.0306, grad_fn=<AddBackward0>)\n","tensor(0.0448, grad_fn=<AddBackward0>)\n","tensor(0.0297, grad_fn=<AddBackward0>)\n","tensor(0.0271, grad_fn=<AddBackward0>)\n","tensor(0.0244, grad_fn=<AddBackward0>)\n","tensor(0.0227, grad_fn=<AddBackward0>)\n","tensor(0.0238, grad_fn=<AddBackward0>)\n","tensor(0.0211, grad_fn=<AddBackward0>)\n","tensor(0.0220, grad_fn=<AddBackward0>)\n","tensor(0.0210, grad_fn=<AddBackward0>)\n","tensor(0.0187, grad_fn=<AddBackward0>)\n","tensor(0.0178, grad_fn=<AddBackward0>)\n","tensor(0.0166, grad_fn=<AddBackward0>)\n","tensor(0.0158, grad_fn=<AddBackward0>)\n","tensor(0.0178, grad_fn=<AddBackward0>)\n","tensor(0.0158, grad_fn=<AddBackward0>)\n","tensor(0.0151, grad_fn=<AddBackward0>)\n","tensor(0.0167, grad_fn=<AddBackward0>)\n","tensor(0.0136, grad_fn=<AddBackward0>)\n","tensor(0.0132, grad_fn=<AddBackward0>)\n","tensor(0.0140, grad_fn=<AddBackward0>)\n","tensor(0.0122, grad_fn=<AddBackward0>)\n","tensor(0.0128, grad_fn=<AddBackward0>)\n","tensor(0.0118, grad_fn=<AddBackward0>)\n","tensor(0.0116, grad_fn=<AddBackward0>)\n","tensor(0.0121, grad_fn=<AddBackward0>)\n","tensor(0.0106, grad_fn=<AddBackward0>)\n","tensor(0.0104, grad_fn=<AddBackward0>)\n","tensor(0.0132, grad_fn=<AddBackward0>)\n","tensor(0.0108, grad_fn=<AddBackward0>)\n","tensor(0.0095, grad_fn=<AddBackward0>)\n","tensor(0.0092, grad_fn=<AddBackward0>)\n","tensor(0.0094, grad_fn=<AddBackward0>)\n","tensor(0.0095, grad_fn=<AddBackward0>)\n","tensor(0.0085, grad_fn=<AddBackward0>)\n","tensor(0.0086, grad_fn=<AddBackward0>)\n","tensor(0.0105, grad_fn=<AddBackward0>)\n","tensor(0.0080, grad_fn=<AddBackward0>)\n","tensor(0.0076, grad_fn=<AddBackward0>)\n","tensor(0.0089, grad_fn=<AddBackward0>)\n","tensor(0.0076, grad_fn=<AddBackward0>)\n","tensor(0.0067, grad_fn=<AddBackward0>)\n","tensor(0.0064, grad_fn=<AddBackward0>)\n","tensor(0.0061, grad_fn=<AddBackward0>)\n","tensor(0.0069, grad_fn=<AddBackward0>)\n","tensor(0.0116, grad_fn=<AddBackward0>)\n","tensor(0.0073, grad_fn=<AddBackward0>)\n","tensor(0.0061, grad_fn=<AddBackward0>)\n","tensor(0.0055, grad_fn=<AddBackward0>)\n","tensor(0.0053, grad_fn=<AddBackward0>)\n","tensor(0.0061, grad_fn=<AddBackward0>)\n","tensor(0.0103, grad_fn=<AddBackward0>)\n","tensor(0.0062, grad_fn=<AddBackward0>)\n","tensor(0.0053, grad_fn=<AddBackward0>)\n","tensor(0.0047, grad_fn=<AddBackward0>)\n","tensor(0.0044, grad_fn=<AddBackward0>)\n","tensor(0.0043, grad_fn=<AddBackward0>)\n","tensor(0.0045, grad_fn=<AddBackward0>)\n","tensor(0.0061, grad_fn=<AddBackward0>)\n","tensor(0.0045, grad_fn=<AddBackward0>)\n","tensor(0.0039, grad_fn=<AddBackward0>)\n","tensor(0.0037, grad_fn=<AddBackward0>)\n","tensor(0.0036, grad_fn=<AddBackward0>)\n","tensor(0.0155, grad_fn=<AddBackward0>)\n","tensor(0.0053, grad_fn=<AddBackward0>)\n","tensor(0.0043, grad_fn=<AddBackward0>)\n","tensor(0.0034, grad_fn=<AddBackward0>)\n","tensor(0.0032, grad_fn=<AddBackward0>)\n","tensor(0.0030, grad_fn=<AddBackward0>)\n","tensor(0.0031, grad_fn=<AddBackward0>)\n","tensor(0.0055, grad_fn=<AddBackward0>)\n","tensor(0.0047, grad_fn=<AddBackward0>)\n","tensor(0.0034, grad_fn=<AddBackward0>)\n","tensor(0.0029, grad_fn=<AddBackward0>)\n","tensor(0.0026, grad_fn=<AddBackward0>)\n","tensor(0.0025, grad_fn=<AddBackward0>)\n","tensor(0.0171, grad_fn=<AddBackward0>)\n","tensor(0.0040, grad_fn=<AddBackward0>)\n","tensor(0.0033, grad_fn=<AddBackward0>)\n","tensor(0.0024, grad_fn=<AddBackward0>)\n","tensor(0.0022, grad_fn=<AddBackward0>)\n","tensor(0.0047, grad_fn=<AddBackward0>)\n","tensor(0.0068, grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UjiDkxIrGJOp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"efa21a56-86db-4985-91fe-82ef66818013","executionInfo":{"status":"ok","timestamp":1572681492478,"user_tz":-540,"elapsed":447,"user":{"displayName":"김태웅","photoUrl":"","userId":"06196795162879697734"}}},"source":["start = torch.zeros(1, n_letters)\n","start[:, -2] = 1\n","\n","with torch.no_grad():\n","  hidden = rnn.init_hidden()\n","  # 입력으로 start 토큰 전달\n","  input_ = start\n","  # output string에 문자열 계속 붙여줌\n","  output_string = ''\n","\n","  for i in range(len(string)):\n","    output, hidden = rnn.forward(input_, hidden)\n","    # 결과값을 문자로 바꿔서 output_string에 붙여줌\n","    output_string += onehot_to_word(output.data)\n","    # 이번 결괏값이 다음의 입력값\n","    input_ = output\n","\n","print(output_string)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["hello pytorch. moyoooongnnpyongonggnngombrrmloe cnyonaoaalrmbrmelnnl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2tbEqkMmHyXR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}